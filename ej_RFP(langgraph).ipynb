{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan-and-Execute란 무엇인가?\n",
    "\n",
    "\"plan-and-execute\"는 다음과 같은 특징을 갖는 접근 방식입니다.\n",
    "\n",
    "- **장기 계획 수립**: 복잡한 작업을 수행하기 전에 큰 그림을 그리는 장기 계획을 수립합니다.\n",
    "- **단계별 실행 및 재계획**: 세운 계획을 단계별로 실행하고, 각 단계가 완료될 때마다 계획이 여전히 유효한지 검토한 뒤 수정할 수 있습니다.\n",
    "  \n",
    "이 방식은 [Plan-and-Solve 논문](https://arxiv.org/abs/2305.04091)과 [Baby-AGI 프로젝트](https://github.com/yoheinakajima/babyagi)에서 영감을 받았습니다. 전통적인 [ReAct 스타일](https://arxiv.org/abs/2210.03629)의 에이전트는 한 번에 한 단계씩 생각하는 반면, \"plan-and-execute\"는 명시적이고 장기적인 계획을 강조합니다.\n",
    "\n",
    "**장점**:\n",
    "1. **명시적인 장기 계획**: 강력한 LLM조차도 한 번에 장기 계획을 처리하는 데 어려움을 겪을 수 있습니다. 명시적으로 장기 계획을 수립함으로써, 보다 안정적인 진행이 가능합니다.\n",
    "2. **효율적인 모델 사용**: 계획 단계에서는 더 큰/강력한 모델을 사용하고, 실행 단계에서는 상대적으로 작은/약한 모델을 사용함으로써 자원 소비를 최적화할 수 있습니다.\n",
    "\n",
    "\n",
    "by. 테디노트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 모델명 정의\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도구 정의\n",
    "\n",
    "사용할 도구를 먼저 정의합니다. 이 간단한 예제에서는 `Tavily`를 통해 제공되는 내장 검색 도구를 사용할 것입니다. 그러나 직접 도구를 만드는 것도 매우 쉽습니다. \n",
    "\n",
    "자세한 내용은 [도구(Tools)](https://wikidocs.net/262582) 문서를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 초기화\n",
    "tools = [TavilySearch(max_results=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Chroma DB...\n",
      "Found 13 PDF files\n",
      "Loading 01_2023미융파_RFP_메타초음파_최종.pdf...\n",
      "Loading 02_2023미융파_RFP_탄소중립_최종.pdf...\n",
      "Loading 03_2023미융파_RFP_이종집적_최종.pdf...\n",
      "Loading 04_2023미융파_RFP_스마트건설_최종.pdf...\n",
      "Loading 05_2023미융파_RFP_광통신_최종.pdf...\n",
      "Loading 2023-과학기술인문사회융합연구-02.pdf...\n",
      "Loading 2023-브릿지융합연구-03.pdf...\n",
      "Loading 2023-브릿지융합연구-04.pdf...\n",
      "Loading 2023-브릿지융합연구-05.pdf...\n",
      "Loading 2023-브릿지융합연구-06.pdf...\n",
      "Loading 2023-브릿지융합연구-07.pdf...\n",
      "Loading 2023년도 제2차 범부처전주기의료기기연구개발사업 신규지원 대상과제 과제제안요청서(RFP).pdf...\n",
      "Loading 붙임 1. 2022년도 제2차 범부처전주기의료기기연구개발사업 신규지원 대상과제 제안요청서(RFP).pdf...\n",
      "Created 325 text chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46fb00d02154bbb9b235c29bfb764fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cbe4a390d94e51aafdbb8a04f47a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46dface8b504d54b542f4f91793793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbcb0e5639243a6ad758adb0a8039f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dff635784ca4ee9b8db0fdb1c32fdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db51f5a7b31247a690a0dbe5e9183d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e0ea99dab64c5ead726dc401fac492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42050a6c251c4194b6e1a2ed03de8cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63225fd0f7f4f019e4893357cf0f5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d68dddee0b4fb1afd4e628f228f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized Chroma DB and retriever\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from typing import List\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# DB 저장 경로 설정\n",
    "PERSIST_DIRECTORY = os.path.join('C:/Users/user/Documents/langchain-kr/어진_RFP', 'db')\n",
    "PDF_DIRECTORY = 'C:/Users/user/Documents/langchain-kr/어진_RFP'\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"HuggingFace 임베딩 모델 설정\"\"\"\n",
    "    model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}  # CPU 사용\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    \n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        cache_folder=\"./cache/\"\n",
    "    )\n",
    "\n",
    "def load_all_pdfs() -> List[str]:\n",
    "    \"\"\"폴더 내의 모든 PDF 파일 경로를 반환\"\"\"\n",
    "    pdf_pattern = os.path.join(PDF_DIRECTORY, '*.pdf')\n",
    "    return glob.glob(pdf_pattern)\n",
    "\n",
    "def load_and_persist_pdfs():\n",
    "    \"\"\"모든 PDF를 로드하고 Chroma DB에 저장\"\"\"\n",
    "    pdf_files = load_all_pdfs()\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Loading {os.path.basename(pdf_path)}...\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    # 텍스트 분할 (청크 크기 조정)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # 더 작은 청크 크기로 조정\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    splits = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} text chunks\")\n",
    "    \n",
    "    # HuggingFace 임베딩 사용\n",
    "    embeddings = get_embeddings()\n",
    "    \n",
    "    # Chroma DB에 저장\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def load_existing_db():\n",
    "    \"\"\"기존 DB 로드\"\"\"\n",
    "    embeddings = get_embeddings()\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def get_vectorstore():\n",
    "    \"\"\"DB가 없으면 생성, 있으면 로드\"\"\"\n",
    "    if not os.path.exists(PERSIST_DIRECTORY):\n",
    "        print(\"Creating new Chroma DB...\")\n",
    "        return load_and_persist_pdfs()\n",
    "    else:\n",
    "        print(\"Loading existing Chroma DB...\")\n",
    "        return load_existing_db()\n",
    "\n",
    "# Chroma DB 초기화\n",
    "try:\n",
    "    vectorstore = get_vectorstore()\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}\n",
    "    )\n",
    "    print(\"Successfully initialized Chroma DB and retriever\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Chroma DB: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "RFP-Assistant\n",
      "Loading existing Chroma DB...\n",
      "\n",
      "=== RFP Assistant 시작 ===\n",
      "'exit'를 입력하면 대화가 종료됩니다.\n",
      "질문이나 RFP 작성 요청을 입력해주세요.\n",
      "\n",
      "\n",
      "AI: RFP(Request for Proposal) 양식에는 다음과 같은 핵심 요소들이 포함되어야 합니다. 각 요소는 명확하고 구체적으로 작성되어야 하며, 제안자들이 과제의 요구사항과 기대치를 명확히 이해할 수 있도록 해야 합니다.\n",
      "\n",
      "1. **문제 정의 및 추진 배경**\n",
      "   - 현재 해결하고자 하는 문제의 정의와 그 문제의 중요성, 시급성에 대한 설명이 필요합니다.\n",
      "   - 문제 발생의 배경과 맥락을 설명하여 제안자들이 문제의 근본 원인을 이해할 수 있도록 합니다.\n",
      "   - 관련된 기존 연구나 프로젝트의 결과 및 한계점도 포함될 수 있습니다.\n",
      "\n",
      "2. **연구 목표 (최종/세부)**\n",
      "   - 연구의 최종 목표와 이를 달성하기 위한 세부 목표를 명확히 기술합니다.\n",
      "   - 목표는 구체적이고 측정 가능해야 하며, SMART(구체적, 측정 가능, 달성 가능, 관련성, 시간 제한) 원칙을 따르는 것이 좋습니다.\n",
      "\n",
      "3. **연구 내용 및 범위**\n",
      "   - 연구가 다루어야 할 주요 내용과 범위를 명확히 정의합니다.\n",
      "   - 연구의 범위는 시간적, 공간적, 주제적 한계를 포함하여 명확히 설정해야 합니다.\n",
      "   - 필요한 경우, 연구 방법론이나 접근 방식에 대한 지침을 제공할 수 있습니다.\n",
      "\n",
      "4. **기대 성과 및 평가 지표**\n",
      "   - 연구가 완료되었을 때 기대되는 성과를 구체적으로 기술합니다.\n",
      "   - 성과를 평가할 수 있는 지표를 제시하여, 연구의 성공 여부를 객관적으로 판단할 수 있도록 합니다.\n",
      "   - 예를 들어, 기술 개발의 경우 기술 성숙도(TRL) 수준, 경제적 효과, 사회적 영향 등을 포함할 수 있습니다.\n",
      "\n",
      "5. **특기 사항 (제약조건, 필수 요구사항 등)**\n",
      "   - 연구 수행 시 고려해야 할 제약조건이나 필수 요구사항을 명시합니다.\n",
      "   - 예산, 일정, 인력 구성, 협력 기관 등과 관련된 구체적인 요구사항을 포함할 수 있습니다.\n",
      "   - 법적, 윤리적, 환경적 고려사항도 포함될 수 있습니다.\n",
      "\n",
      "이러한 요소들을 포함하여 RFP를 작성하면, 제안자들이 과제의 요구사항을 명확히 이해하고, 적절한 제안서를 제출할 수 있도록 도울 수 있습니다.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: 질문이나 요청사항을 명확히 제시해주시면, 해당 내용을 바탕으로 RFP 작성이나 분석에 필요한 정보를 제공해드리겠습니다. 예를 들어, 특정 RFP의 문제 정의를 어떻게 설정해야 하는지, 연구 목표를 어떻게 구체화할 수 있는지, 또는 평가 지표 설정에 대한 조언이 필요하신지 등을 알려주시면 도움이 됩니다. 추가적인 세부사항을 제공해주시면 더욱 구체적이고 유용한 답변을 드릴 수 있습니다.\n",
      "--------------------------------------------------\n",
      "\n",
      "대화를 종료합니다. 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from langchain_teddynote.tools import TavilySearch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# API 키 로드\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith 설정\n",
    "logging.langsmith(\"RFP-Assistant\")\n",
    "\n",
    "# 모델명 정의\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# DB 저장 경로 설정\n",
    "PERSIST_DIRECTORY = os.path.join('C:/Users/user/Documents/langchain-kr/어진_RFP', 'db')\n",
    "PDF_DIRECTORY = 'C:/Users/user/Documents/langchain-kr/어진_RFP'\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"HuggingFace 임베딩 모델 설정\"\"\"\n",
    "    model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    \n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        cache_folder=\"./cache/\"\n",
    "    )\n",
    "\n",
    "def load_all_pdfs() -> List[str]:\n",
    "    \"\"\"폴더 내의 모든 PDF 파일 경로를 반환\"\"\"\n",
    "    pdf_pattern = os.path.join(PDF_DIRECTORY, '*.pdf')\n",
    "    return glob.glob(pdf_pattern)\n",
    "\n",
    "def load_and_persist_pdfs():\n",
    "    \"\"\"모든 PDF를 로드하고 Chroma DB에 저장\"\"\"\n",
    "    pdf_files = load_all_pdfs()\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Loading {os.path.basename(pdf_path)}...\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    splits = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} text chunks\")\n",
    "    \n",
    "    embeddings = get_embeddings()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def load_existing_db():\n",
    "    \"\"\"기존 DB 로드\"\"\"\n",
    "    embeddings = get_embeddings()\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def get_vectorstore():\n",
    "    \"\"\"DB가 없으면 생성, 있으면 로드\"\"\"\n",
    "    if not os.path.exists(PERSIST_DIRECTORY):\n",
    "        print(\"Creating new Chroma DB...\")\n",
    "        return load_and_persist_pdfs()\n",
    "    else:\n",
    "        print(\"Loading existing Chroma DB...\")\n",
    "        return load_existing_db()\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"문서 포맷팅\"\"\"\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# 메모리 설정\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    output_key=\"output\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "def load_memory(input_dict):\n",
    "    \"\"\"메모리 로드\"\"\"\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "class PDFIngestor:\n",
    "    def __init__(self):\n",
    "        self.persist_directory = PERSIST_DIRECTORY\n",
    "        self.pdf_directory = PDF_DIRECTORY\n",
    "\n",
    "    def get_retriever(self):\n",
    "        \"\"\"벡터 스토어에서 리트리버 생성\"\"\"\n",
    "        vectorstore = get_vectorstore()\n",
    "        return vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "\n",
    "class Chain:\n",
    "    def __init__(self, retriever):\n",
    "        # LLM 정의\n",
    "        llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "        \n",
    "        # 프롬프트 템플릿 정의\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"당신은 RFP(Request for Proposal) 작성과 분석을 돕는 전문 어시스턴트입니다.\n",
    "            \n",
    "            다음과 같은 RFP의 핵심 요소들을 중심으로 답변해주세요:\n",
    "            1. 문제 정의 및 추진 배경\n",
    "            2. 연구 목표 (최종/세부)\n",
    "            3. 연구 내용 및 범위\n",
    "            4. 기대 성과 및 평가 지표\n",
    "            5. 특기 사항 (제약조건, 필수 요구사항 등)\n",
    "            \n",
    "            답변 시에는 구체적이고 실현 가능한 제안을 제시하되, 혁신성과 도전성도 균형있게 고려해주세요.\n",
    "            항상 한국어로 답변하며, 가능한 명확한 근거와 예시를 포함해주세요.\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"[참고 문서]\\n{context}\\n\\n[질문/요청사항]\\n{query}\")\n",
    "        ])\n",
    "        \n",
    "        # 체인 정의\n",
    "        self.chain = (\n",
    "            {\n",
    "                'context': retriever | format_docs,\n",
    "                'query': RunnablePassthrough()\n",
    "            }\n",
    "            | RunnablePassthrough.assign(history=load_memory)\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def astream(self, query):\n",
    "        \"\"\"스트리밍 방식으로 응답 생성\"\"\"\n",
    "        result = ''\n",
    "        for chunk in self.chain.stream(query):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            result += chunk\n",
    "\n",
    "        memory.save_context(\n",
    "            {\"input\": query},\n",
    "            {\"output\": result},\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    # PDF 처리기 초기화 및 리트리버 생성\n",
    "    pdf_ingestor = PDFIngestor()\n",
    "    retriever = pdf_ingestor.get_retriever()\n",
    "\n",
    "    # Chain 초기화\n",
    "    chain = Chain(retriever)\n",
    "\n",
    "    print(\"\\n=== RFP Assistant 시작 ===\")\n",
    "    print(\"'exit'를 입력하면 대화가 종료됩니다.\")\n",
    "    print(\"질문이나 RFP 작성 요청을 입력해주세요.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\\n대화를 종료합니다. 감사합니다!\")\n",
    "            break\n",
    "\n",
    "        print(\"\\nAI: \", end=\"\")\n",
    "        chain.astream(user_input)\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph: 상태 기반 AI 워크플로우 프레임워크\n",
    "\n",
    "## 개요\n",
    "LangGraph는 LangChain 생태계의 일부로, 상태 기반의 AI 워크플로우를 구축하기 위한 프레임워크입니다. 복잡한 AI 시스템을 모듈화된 그래프 구조로 구현할 수 있게 해줍니다.\n",
    "\n",
    "## 핵심 개념\n",
    "\n",
    "### 1. 상태 (State)\n",
    "- 워크플로우의 현재 상태를 나타내는 데이터 구조\n",
    "- TypedDict를 사용하여 정의\n",
    "- 노드 간 데이터 전달의 기반\n",
    "\n",
    "### 2. 노드 (Node)\n",
    "- 워크플로우의 개별 작업 단위\n",
    "- 상태를 입력받아 처리하고 업데이트된 상태를 반환\n",
    "- 순수 함수로 구현 권장\n",
    "\n",
    "### 3. 엣지 (Edge)\n",
    "- 노드 간의 연결을 정의\n",
    "- 데이터 흐름의 방향을 결정\n",
    "- 조건부 분기 가능\n",
    "\n",
    "### 4. 체크포인팅 (Checkpointing)\n",
    "- 실행 상태 추적 및 저장\n",
    "- 디버깅 및 모니터링 지원\n",
    "- 실행 재개 가능성 제공\n",
    "\n",
    "## 주요 특징\n",
    "\n",
    "### 1. 모듈성\n",
    "- 각 노드는 독립적으로 개발 및 테스트 가능\n",
    "- 재사용 가능한 컴포넌트로 설계\n",
    "- 유지보수 용이성 향상\n",
    "\n",
    "### 2. 유연성\n",
    "- 동적 워크플로우 구성 가능\n",
    "- 조건부 분기 처리\n",
    "- 병렬 실행 지원\n",
    "\n",
    "### 3. 추적성\n",
    "- 실행 흐름 모니터링\n",
    "- 상태 변화 추적\n",
    "- 디버깅 용이성\n",
    "\n",
    "### 4. 확장성\n",
    "- 새로운 노드 쉽게 추가\n",
    "- 기존 워크플로우 수정 용이\n",
    "- 복잡한 시스템 구현 가능\n",
    "\n",
    "## 일반적인 사용 사례\n",
    "\n",
    "### 1. 대화형 시스템\n",
    "- 멀티턴 대화 관리\n",
    "- 컨텍스트 유지\n",
    "- 대화 흐름 제어\n",
    "\n",
    "### 2. 복잡한 추론 시스템\n",
    "- Plan-and-Execute 패턴\n",
    "- ReAct 패턴\n",
    "- 멀티스텝 추론\n",
    "\n",
    "### 3. 워크플로우 자동화\n",
    "- 문서 처리 파이프라인\n",
    "- 데이터 변환 및 강화\n",
    "- 품질 관리 프로세스\n",
    "\n",
    "## 모범 사례\n",
    "\n",
    "### 1. 상태 설계\n",
    "- 명확한 타입 정의\n",
    "- 최소한의 필요 데이터만 포함\n",
    "- 불변성 고려\n",
    "\n",
    "### 2. 노드 구현\n",
    "- 단일 책임 원칙 준수\n",
    "- 부작용 최소화\n",
    "- 에러 처리 포함\n",
    "\n",
    "### 3. 워크플로우 구성\n",
    "- 논리적 흐름 설계\n",
    "- 적절한 에러 처리 경로\n",
    "- 확장성 고려\n",
    "\n",
    "### 4. 테스트 및 디버깅\n",
    "- 단위 테스트 작성\n",
    "- 통합 테스트 구현\n",
    "- 로깅 전략 수립\n",
    "\n",
    "## 장점\n",
    "\n",
    "1. **구조화된 접근**\n",
    "   - 복잡한 로직을 관리 가능한 단위로 분할\n",
    "   - 명확한 데이터 흐름\n",
    "   - 코드 구조 개선\n",
    "\n",
    "2. **유지보수성**\n",
    "   - 모듈화된 구조\n",
    "   - 독립적인 컴포넌트\n",
    "   - 테스트 용이성\n",
    "\n",
    "3. **확장성**\n",
    "   - 새로운 기능 추가 용이\n",
    "   - 기존 로직 수정 간편\n",
    "   - 재사용 가능한 컴포넌트\n",
    "\n",
    "4. **디버깅**\n",
    "   - 상태 변화 추적\n",
    "   - 실행 흐름 모니터링\n",
    "   - 오류 지점 식별 용이\n",
    "\n",
    "## 결론\n",
    "\n",
    "LangGraph는 복잡한 AI 시스템을 구조화된 방식으로 구현할 수 있게 해주는 강력한 도구입니다. 상태 기반 접근방식과 모듈화된 구조는 확장 가능하고 유지보수하기 쉬운 시스템을 구축하는데 매우 효과적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\user\\appdata\\local\\pypoetry\\cache\\virtualenvs\\langchain-kr-kbdaaxj8-py3.11\\lib\\site-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install graphviz > 그래프를 보기 위한 라이브러리 (필요시에만 진행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# LangGraph 관련 임포트\n",
    "from langgraph.graph import StateGraph, START, END  # 그래프 구조 정의를 위한 기본 클래스\n",
    "from langgraph.checkpoint.memory import MemorySaver  # 실행 상태 추적을 위한 체크포인터\n",
    "from langchain_core.runnables import RunnableConfig  # 실행 설정을 위한 클래스\n",
    "from uuid import uuid4\n",
    "from typing import Annotated, TypedDict, List\n",
    "from typing_extensions import TypedDict\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# API 키 로드\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith 설정\n",
    "logging.langsmith(\"RFP-Assistant\")\n",
    "\n",
    "# 모델명 정의\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# DB 저장 경로 설정\n",
    "PERSIST_DIRECTORY = os.path.join('C:/Users/user/Documents/langchain-kr/어진_RFP', 'db')\n",
    "PDF_DIRECTORY = 'C:/Users/user/Documents/langchain-kr/어진_RFP'\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"HuggingFace 임베딩 모델 설정\"\"\"\n",
    "    model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    \n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        cache_folder=\"./cache/\"\n",
    "    )\n",
    "\n",
    "def load_all_pdfs() -> List[str]:\n",
    "    \"\"\"폴더 내의 모든 PDF 파일 경로를 반환\"\"\"\n",
    "    pdf_pattern = os.path.join(PDF_DIRECTORY, '*.pdf')\n",
    "    return glob.glob(pdf_pattern)\n",
    "\n",
    "def load_and_persist_pdfs():\n",
    "    \"\"\"모든 PDF를 로드하고 Chroma DB에 저장\"\"\"\n",
    "    pdf_files = load_all_pdfs()\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Loading {os.path.basename(pdf_path)}...\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    splits = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} text chunks\")\n",
    "    \n",
    "    embeddings = get_embeddings()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def load_existing_db():\n",
    "    \"\"\"기존 DB 로드\"\"\"\n",
    "    embeddings = get_embeddings()\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def get_vectorstore():\n",
    "    \"\"\"DB가 없으면 생성, 있으면 로드\"\"\"\n",
    "    if not os.path.exists(PERSIST_DIRECTORY):\n",
    "        print(\"Creating new Chroma DB...\")\n",
    "        return load_and_persist_pdfs()\n",
    "    else:\n",
    "        print(\"Loading existing Chroma DB...\")\n",
    "        return load_existing_db()\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"문서 포맷팅\"\"\"\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LangGraph의 상태를 정의하는 클래스\n",
    "# TypedDict를 사용하여 상태의 구조를 명시적으로 정의\n",
    "class RFPState(TypedDict):\n",
    "    \"\"\"RFP Assistant의 상태를 정의하는 클래스\n",
    "    \n",
    "    Attributes:\n",
    "        query: 사용자의 입력 질문\n",
    "        context: 검색된 문서 컨텍스트\n",
    "        chat_history: 대화 히스토리 리스트\n",
    "        response: AI의 최종 응답\n",
    "    \"\"\"\n",
    "    query: Annotated[str, \"사용자 입력\"]\n",
    "    context: Annotated[str, \"검색된 문서 컨텍스트\"]\n",
    "    chat_history: Annotated[list, \"대화 히스토리\"]\n",
    "    response: Annotated[str, \"최종 응답\"]\n",
    "\n",
    "# LangGraph 노드 함수 정의\n",
    "def retrieve_context(state: RFPState):\n",
    "    \"\"\"문서 검색 노드\n",
    "    \n",
    "    Args:\n",
    "        state: 현재 워크플로우 상태\n",
    "        \n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 (context 키 포함)\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    retriever = get_vectorstore().as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}\n",
    "    )\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = format_docs(docs)\n",
    "    return {\"context\": context}\n",
    "\n",
    "def generate_response(state: RFPState):\n",
    "    \"\"\"응답 생성 노드\n",
    "    \n",
    "    Args:\n",
    "        state: 현재 워크플로우 상태\n",
    "        \n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 (response 키 포함)\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"당신은 RFP(Request for Proposal) 작성과 분석을 돕는 전문 어시스턴트입니다.\n",
    "        \n",
    "        다음과 같은 RFP의 핵심 요소들을 중심으로 답변해주세요:\n",
    "        1. 문제 정의 및 추진 배경\n",
    "        2. 연구 목표 (최종/세부)\n",
    "        3. 연구 내용 및 범위\n",
    "        4. 기대 성과 및 평가 지표\n",
    "        5. 특기 사항 (제약조건, 필수 요구사항 등)\n",
    "        \n",
    "        답변 시에는 구체적이고 실현 가능한 제안을 제시하되, 혁신성과 도전성도 균형있게 고려해주세요.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"[참고 문서]\\n{context}\\n\\n[질문/요청사항]\\n{query}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\n",
    "        \"context\": state[\"context\"],\n",
    "        \"query\": state[\"query\"],\n",
    "        \"chat_history\": state[\"chat_history\"]\n",
    "    })\n",
    "    \n",
    "    return {\"response\": response}\n",
    "\n",
    "def update_history(state: RFPState):\n",
    "    \"\"\"대화 히스토리 업데이트 노드\n",
    "    \n",
    "    Args:\n",
    "        state: 현재 워크플로우 상태\n",
    "        \n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 (chat_history 키 포함)\n",
    "    \"\"\"\n",
    "    current_history = state[\"chat_history\"]\n",
    "    current_history.extend([\n",
    "        (\"human\", state[\"query\"]),\n",
    "        (\"assistant\", state[\"response\"])\n",
    "    ])\n",
    "    return {\"chat_history\": current_history}\n",
    "\n",
    "def create_rfp_graph():\n",
    "    \"\"\"RFP Assistant 그래프 생성\n",
    "    \n",
    "    LangGraph를 사용하여 워크플로우를 정의하고 컴파일합니다.\n",
    "    \n",
    "    Returns:\n",
    "        compiled_graph: 컴파일된 LangGraph 워크플로우\n",
    "    \"\"\"\n",
    "    # StateGraph 인스턴스 생성 (상태 타입 지정)\n",
    "    workflow = StateGraph(RFPState)\n",
    "    \n",
    "    # 노드 추가 (각 작업 단계 정의)\n",
    "    workflow.add_node(\"retrieve\", retrieve_context)  # 문서 검색 노드\n",
    "    workflow.add_node(\"generate\", generate_response)  # 응답 생성 노드\n",
    "    workflow.add_node(\"update_history\", update_history)  # 히스토리 업데이트 노드\n",
    "    \n",
    "    # 엣지 추가 (노드 간 연결 정의)\n",
    "    workflow.add_edge(START, \"retrieve\")  # 시작 → 문서 검색\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")  # 문서 검색 → 응답 생성\n",
    "    workflow.add_edge(\"generate\", \"update_history\")  # 응답 생성 → 히스토리 업데이트\n",
    "    workflow.add_edge(\"update_history\", END)  # 히스토리 업데이트 → 종료\n",
    "    \n",
    "    # 그래프 컴파일 (체크포인터 설정)\n",
    "    return workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "def visualize_workflow():\n",
    "    \"\"\"\n",
    "    LangGraph 워크플로우를 시각화하는 함수\n",
    "    \n",
    "    이 함수는 다음을 수행합니다:\n",
    "    1. 워크플로우 그래프 생성\n",
    "    2. Graphviz를 사용한 시각화\n",
    "    3. 상세 정보 표시 (xray 모드)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from graphviz import Digraph\n",
    "        \n",
    "        # 워크플로우 그래프 생성\n",
    "        workflow = create_rfp_graph()\n",
    "        \n",
    "        # 그래프 객체 생성\n",
    "        dot = Digraph(comment='RFP Assistant Workflow')\n",
    "        dot.attr(rankdir='LR')\n",
    "        \n",
    "        # 노드 스타일 설정\n",
    "        dot.attr('node', shape='box', style='rounded', fontname='Arial')\n",
    "        \n",
    "        # 노드 색상 설정\n",
    "        node_colors = {\n",
    "            'START': 'lightgreen',\n",
    "            'END': 'lightpink',\n",
    "            'retrieve': 'lightblue',\n",
    "            'generate': 'lightyellow',\n",
    "            'update_history': 'lightgrey'\n",
    "        }\n",
    "        \n",
    "        # 노드 추가 (색상과 함께)\n",
    "        for node, color in node_colors.items():\n",
    "            shape = 'circle' if node in ['START', 'END'] else 'box'\n",
    "            dot.node(node, node, shape=shape, style='filled', fillcolor=color)\n",
    "        \n",
    "        # 엣지 추가\n",
    "        dot.edge('START', 'retrieve')\n",
    "        dot.edge('retrieve', 'generate')\n",
    "        dot.edge('generate', 'update_history')\n",
    "        dot.edge('update_history', 'END')\n",
    "        \n",
    "        # 그래프 저장\n",
    "        try:\n",
    "            dot.render(\"rfp_workflow\", format=\"png\", cleanup=True)\n",
    "            print(\"워크플로우 그래프가 'rfp_workflow.png'로 저장되었습니다.\")\n",
    "            return dot\n",
    "        except Exception as e:\n",
    "            print(f\"그래프 저장 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"\\nGraphviz 설치가 필요합니다:\")\n",
    "        print(\"1. pip install graphviz\")\n",
    "        print(\"2. 시스템에 Graphviz 설치:\")\n",
    "        print(\"   - Windows: https://graphviz.org/download/\")\n",
    "        print(\"   - Ubuntu: sudo apt-get install graphviz\")\n",
    "        print(\"   - Mac: brew install graphviz\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n시각화 중 오류 발생: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 시각화 실행 (주피터 노트북 셀에서 실행)\n",
    "if __name__ == \"__main__\":\n",
    "    # 워크플로우 시각화\n",
    "    graph = visualize_workflow()\n",
    "    if graph:\n",
    "        # 그래프를 PNG 파일로 저장\n",
    "        graph.render(\"rfp_workflow\", format=\"png\", cleanup=True)\n",
    "        print(\"워크플로우 그래프가 'rfp_workflow.png'로 저장되었습니다.\")\n",
    "\n",
    "def main():\n",
    "    # 그래프 생성\n",
    "    app = create_rfp_graph()\n",
    "    \n",
    "    # 초기 상태 설정\n",
    "    initial_state = {\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RFP Assistant 시작 ===\")\n",
    "    print(\"'exit'를 입력하면 대화가 종료됩니다.\")\n",
    "    print(\"질문이나 RFP 작성 요청을 입력해주세요.\\n\")\n",
    "    \n",
    "    # LangGraph 체크포인터 설정\n",
    "    # - thread_id: 대화 세션 식별\n",
    "    # - checkpoint_ns: 체크포인트 네임스페이스\n",
    "    # - checkpoint_id: 개별 체크포인트 식별\n",
    "    config = RunnableConfig(\n",
    "        configurable={\n",
    "            \"thread_id\": str(uuid4()),\n",
    "            \"checkpoint_ns\": \"rfp_assistant\",\n",
    "            \"checkpoint_id\": str(uuid4()),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\\n대화를 종료합니다. 감사합니다!\")\n",
    "            break\n",
    "        \n",
    "        # 현재 상태 업데이트\n",
    "        current_state = {\n",
    "            **initial_state,\n",
    "            \"query\": user_input,\n",
    "        }\n",
    "        \n",
    "        # 그래프 실행 (상태와 설정 전달)\n",
    "        print(\"\\nAI: \", end=\"\")\n",
    "        result = app.invoke(current_state, config=config)\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "        # 대화 히스토리 업데이트\n",
    "        initial_state[\"chat_history\"] = result[\"chat_history\"]\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
